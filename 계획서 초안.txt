---

## 1) 프로젝트 개요

### 1.1 프로젝트 한 줄 정의

**브라우저만으로(추가 장비 없이도) “같이 듣고, 같이 부르고, 같이 연주하는” 실시간 음악 공간**을 제공하는 웹 플랫폼.

- **WebRTC**: 저지연 음성/연주 오디오(및 선택적 영상) 전송
- **WebSocket(STOMP)**: 실시간 시그널링/채팅/방 상태/동기화 이벤트 처리
- **MR(반주) 검색/불러오기 + 믹싱**: 노래방/합주 모드의 핵심 차별점

### 1.2 기존 SYNCROOM 대비 개선 의도

- SYNCROOM은 저지연 원격 합주에 강점이 있지만, 일반 사용자 관점에서 **MR 검색/노래방 흐름이 없다**.
- 또 “특정 장비가 있어야 다른 참여자에게 소리를 전달”하는 경험(최소한의 세팅 난이도/오디오 라우팅) 때문에 진입장벽이 생김.
- 따라서 이번 프로젝트는 **웹(브라우저) 기반으로 장비 의존도를 낮추고**, “노래방/스트리밍”까지 한 번에 엮어 **커뮤니티형 음악 공간**으로 확장하는 게 핵심.

> 참고: SYNCROOM은 “광대역 유선 + ASIO 오디오 인터페이스” 등을 권장하며(환경에 따라 장비/드라이버 세팅이 중요) (SyncRoom), 오디오 통신은 P2P 직접 연결 구조를 안내한다. (SyncRoom)
> 

---

## 2) 목표와 성공 기준

### 2.1 핵심 목표(Top 4)

1. **브라우저만으로 방 생성 → 입장 → 마이크 테스트 → 합창/합주/노래방 시작**까지 “막힘 없는” 플로우
2. **MR 검색/선곡/큐 관리** + **가사/타이밍** 제공(노래방 UX)
3. **실시간 스트리밍 스테이션**: 모두가 같은 트랙/영상/상태를 보며 채팅/리액션
4. 네트워크 환경이 다양한 팀원/사용자를 고려한 **WebRTC 안정성(ICE, TURN, 재접속)** 확보

### 2.2 MVP 성공 기준(측정 가능한 형태)

- 2~6명이 같은 방에서 **음성 통화 + 채팅 + MR 재생 동기화**를 끊김 없이 수행
- 사용자가 “내 목소리 + MR”을 **상대에게 들려주기**까지(믹싱) 3분 내 성공
- 방 이탈/재입장, 장치 변경(마이크/스피커), 네트워크 변경에도 **복구 UX 제공**

---

## 3) 서비스 콘셉트와 주요 사용자 시나리오

### 3.1 사용자 역할

- **Host(방장)**: 스테이션/노래방 모드 제어(선곡, 큐, 시작/정지, 권한)
- **Performer(참여자)**: 마이크 참여(보컬/연주), 신청곡/리액션/채팅
- **Listener(청취자)**: 청취/채팅 중심(음성 송출 제한 가능)

### 3.2 대표 시나리오

1. “오늘 같이 노래방 하자”
- 방 생성 → MR 검색/선곡 → 가사 표시 → 카운트다운 → 참여자 마이크 On → 합창
1. “합주/세션”
- 방 생성 → 메트로놈/템포 설정 → 오디오 전송 최적화(에코캔슬 off, AGC 설정) → 세션
1. “스테이션(같이 보기/듣기)”
- 트랙/영상 링크 공유 → 모두 동일 재생 위치로 동기화 → 채팅/리액션/투표

---

## 4) 기능 범위 정의

### 4.1 필수 기능(MVP)

**A. 계정/방**

- 로그인/회원가입(JWT)
- 방 생성/검색/입장(공개/비공개, 인원 제한, 비밀번호)
- 방 내 권한(방장/참여자/청취자), 강퇴/뮤트, 신고

**B. 실시간**

- WebSocket(STOMP) 기반
    - 채팅(코드블럭/이모지/스티커는 옵션)
    - 방 상태(누가 입장/퇴장, 준비 상태, 장치 상태)
    - WebRTC 시그널링(offer/answer/ice)
    - 동기화 이벤트(재생 시작/정지/seek, 카운트다운)

**C. WebRTC 오디오**

- 마이크/스피커 장치 선택 UI
- 오디오 품질 프리셋(대화/노래/연주)
- 네트워크 품질 표시(지터/RTT/패킷로스)

**D. MR(반주)**

- MR 검색(최소: 키워드/아티스트/곡명)
- 큐(대기열) + 방장 승인
- MR 재생 동기화(모두 같은 타임코드)
- “내 목소리 + MR”을 **상대에게 전달**(브라우저에서 믹싱)

### 4.2 확장 기능(2차~3차)

- 녹음/리플레이(방 단위 세션 저장)
- 점수/피치 분석(간단한 음정 그래프)
- 합주용 메트로놈, BPM 공유, 레이턴시 보정
- SFU 도입(인원 확장, 모바일 대응 강화)
- 커뮤니티(클립 공유, 오늘의 스테이션, 태그 검색)

---

## 5) 기술 스택 제안(초안)

### 5.1 Frontend

- React + TypeScript
- shadcn/ui + Tailwind + Radix UI
- WebRTC(브라우저 API) + Web Audio API(믹싱/라우팅)
- 상태관리: Zustand/Redux Toolkit 중 택1(실시간 상태가 많으면 RTK가 유리)

### 5.2 Backend

- Spring Boot (Java 21, Gradle)
- JPA/Hibernate
- WebSocket(STOMP) + JWT 인증
- Redis(선택): 방 상태/동기화 이벤트/세션 캐시

### 5.3 인프라/네트워크

- STUN/TURN: coturn(사실상 필수에 가까움 — NAT 환경 대응)
- 배포: Docker + Nginx(Reverse Proxy) + TLS
- 관측: 로그(ELK/Cloud), 지표(Prometheus) — 여건 되면

---

## 6) 핵심 아키텍처 설계

### 6.1 큰 구조(권장)

- **Backend(Spring)**: 인증/방/권한/채팅/시그널링/동기화 “컨트롤 플레인”
- **Media(WebRTC)**: 초기엔 P2P(mesh) 중심, 확장 시 SFU 별도 서비스로 분리

SYNCROOM도 “방 관리용 서버 + 오디오는 P2P” 분리를 강조한다. ([SyncRoom](https://syncroom.yamaha.com/global/v2/play/faq/?utm_source=chatgpt.com))

=> 팀 프로젝트 MVP로는 이 구조가 현실적(구현 난이도/리스크 대비 성과가 큼).

### 6.2 WebRTC 연결 흐름(요약)

1. 방 입장 → WebSocket 연결(STOMP)
2. 클라이언트가 방의 peer 목록 수신
3. offer/answer 교환 + ICE candidate 교환(시그널링은 WebSocket)
4. P2P 연결 성립 후 오디오 트랙 송수신
5. 연결 품질(Stats API) 측정하여 UI/자동 튜닝

### 6.3 “MR + 마이크”를 상대에게 들려주는 방법(핵심 구현 포인트)

브라우저에서 가능한 현실적 접근 2가지:

**안1) Web Audio 믹싱 후 WebRTC로 송출(권장 MVP)**

- 마이크(MediaStream) → AudioContext 입력
- MR(AudioBufferSource/MediaElement) → AudioContext 입력
- 둘을 GainNode로 믹스 → MediaStreamAudioDestinationNode로 출력 스트림 생성
- 생성된 “혼합 스트림”을 WebRTC 송출 트랙으로 사용
    
    장점: “장비 없이” 노래방 핵심을 구현 가능
    
    주의: 에코/하울링 방지(헤드폰 유도, AEC 설정), 브라우저별 제약 테스트
    

**안2) MR은 모두가 각자 로컬 재생 + 싱크만 맞추고, 음성만 송출**

- MR 파일/스트림을 모두에게 제공하고, 서버 타임 기준으로 시작 시점/seek만 동기화
    
    장점: 상대에게 MR을 굳이 송출하지 않아도 됨(트래픽↓)
    
    단점: “상대가 듣는 MR이 내 MR과 완전 동일” 보장이 어려움(딜레이/디코딩 차)
    

MVP에서는 **안1**이 “노래방” 체감이 더 좋고, 시연 임팩트가 큼.

---

## 7) MR 소스/검색 전략(현실적인 선택지)

여기서 가장 조심해야 할 건 **저작권/이용약관**이야.

- YouTube는 서비스 이용이 약관/정책의 적용을 받으며 ([YouTube](https://www.youtube.com/static?gl=GB&template=terms&utm_source=chatgpt.com)), 임베드 역시 API 약관/정책이 적용된다고 안내한다. ([구글 도움말](https://support.google.com/youtube/answer/171780?hl=en&utm_source=chatgpt.com))
- 한국에서도 “온라인 공연/스트리밍”에 저작권 사용료 이슈가 발생할 수 있다는 취지의 보도/자료가 있다. ([Korea Joongang Daily](https://koreajoongangdaily.joins.com/2022/08/23/business/industry/Korea-copyright-online/20220823144814305.html?utm_source=chatgpt.com))

그래서 “학교/포트폴리오용 팀 프로젝트” 기준으로는 아래처럼 단계화하는 게 안전하고 구현도 깔끔해져:

### 7.1 1차(MVP): 합법 리스크 최소화

- **사용자 로컬 파일 업로드 기반 MR**(wav/mp3) + 방 내부 공유(혹은 각자 로컬 재생 + 동기화)
- 또는 **저작권 프리/CC 라이선스 MR**만 검색(샘플 데이터셋 구축)

### 7.2 2차: 외부 검색 연동(정책 준수형)

- “검색”은 하되, 실제 재생은 **공식 임베드/정책 준수 방식**으로 제한(예: YouTube IFrame Player)
- 단, **임베드된 YouTube 오디오를 WebRTC로 ‘재송출’**하는 형태는 정책/권리 이슈 소지가 커서(특히 서비스 형태가 커질수록) 기획 단계에서 선을 정하는 게 좋아.

### 7.3 3차: 상용/정식 서비스 관점

- MR 제공사/음원 유통사 라이선스 계약 또는 합법 MR API 도입(현실적으로 팀프로젝트 범위를 넘을 수 있음)

---

## 8) 데이터 모델(초안)

- User(id, email, nickname, provider, createdAt…)
- Room(id, title, mode(STATION/KARAOKE/SESSION), visibility, passwordHash, hostId, createdAt…)
- RoomMember(roomId, userId, role, joinedAt, muted, deviceInfo…)
- ChatMessage(id, roomId, userId, type(TEXT/SYSTEM), content, createdAt)
- Track(id, sourceType(UPLOAD/CC/EMBED), title, artist, duration, url/meta)
- QueueItem(id, roomId, trackId, requestedBy, status(PENDING/APPROVED/PLAYING/DONE), order, createdAt)
- PlaybackState(roomId, trackId, positionMs, isPlaying, updatedAt) // 실시간은 캐시(Redis), 저장은 선택
- Recording(id, roomId, ownerId, storageUrl, createdAt) // 2차~

---

## 9) 개발 단계 제안(1차/2차/3차)

### 1차: “작동하는 노래방/스테이션” (시연 가능한 MVP)

- WebSocket(STOMP) + JWT + 방/채팅
- WebRTC 오디오 P2P 연결 + 장치 선택/테스트
- MR: 로컬 업로드(또는 CC 트랙) + 큐 + 동기화
- 마이크+MR 믹싱 송출(핵심 차별점)

### 2차: “품질/안정성”

- TURN 적용/최적화, 재접속/네트워크 전환 복구
- 품질 프리셋/Stats 기반 안내(“지금은 유선 권장”, “지터 높음”)
- 녹음(클라이언트/서버 중 택) 또는 하이라이트 클립
- 모더레이션(신고/차단/권한 강화)

### 3차: “확장성과 커뮤니티”

- SFU 도입(인원 확장), 모바일 개선
- 공개 스테이션/클립 피드/검색/태그
- 추천(자주 부르는 곡, 방 분위기 태그 기반)

---

## 10) 역할 분담(5인 기준 예시)

1. **Realtime/WebRTC 리드**
- 시그널링 프로토콜, ICE/TURN, 오디오 품질 프리셋, Stats/복구 로직
1. **Backend 리드(Spring)**
- 인증/JWT, 방/권한, WebSocket(STOMP) 엔드포인트, 데이터 모델/JPA
1. **Frontend 리드(React)**
- 라우팅/상태관리, 방 UI/디바이스 설정 UX, shadcn 컴포넌트 기반 화면 설계
1. **MR/미디어 파이프라인 담당**
- MR 소스 전략(업로드/CC/임베드), Web Audio 믹싱, 동기화(카운트다운/seek)
1. **DevOps/QA(겸 PM)**
- Docker/Nginx/TLS, 배포 파이프라인, 테스트 시나리오/부하/장애 대응 문서화

> 팀 상황에 따라 4~5번은 합칠 수도 있는데, WebRTC 프로젝트는 “붙여서 되는 것”보다 “안정성/재현 가능한 테스트”가 훨씬 중요해서 QA/DevOps 역할을 가볍게라도 두는 게 진짜 도움 돼.
> 

---

## 11) 주요 리스크와 대응

### 11.1 WebRTC 연결 실패/NAT 이슈

- 대응: coturn(TURN) 구축, ICE 상태 모니터링, 실패 시 재시도/대체 경로 안내

### 11.2 에코/하울링, 노래방에서의 체감 품질

- 대응: 기본은 헤드폰 권장 UI + AEC/AGC 옵션 프리셋 제공
- “노래/연주 모드”에서 AEC가 악영향 줄 수 있으니 모드별 튜닝

### 11.3 MR/음원 저작권 및 플랫폼 정책

- 대응: MVP는 업로드/CC 중심으로 설계, 임베드/외부 음원은 정책 준수 범위에서만

### 11.4 동기화(“다 같이 같은 타이밍”) 난이도

- 대응: 서버 타임 기준 카운트다운 + 주기적 drift 보정(소폭 seek)
- WebRTC stats(RTT) 기반 보정값 적용(2차)

---

## 12) 산출물(문서/데모 기준)

- 요구사항 정의서(모드별 유저 스토리)
- 시퀀스 다이어그램: “방 입장 → WebRTC 연결”, “MR 큐 → 동기화 재생”
- ERD(초안 → 확정)
- API 명세(REST + STOMP topic/queue 규약)
- 테스트 체크리스트(브라우저/네트워크/장치 변경)
- 데모 시나리오 3종(노래방/합주/스테이션)

---

## 13) 화면/UX 설계(초안)

### 13.1 핵심 화면 목록

- **랜딩/로그인**: 소셜/이메일 로그인, 서비스 소개, 데모 영상
- **로비**: 공개 방 목록/검색, 최근 방, 내 즐겨찾기
- **방 생성**: 모드(노래방/스테이션/세션), 공개/비공개, 인원 제한, 비밀번호
- **방 내부(노래방/세션)**: 참여자 리스트, 마이크 상태, 채팅, MR 큐, 가사 뷰, 카운트다운
- **방 내부(스테이션)**: 현재 트랙/영상, 진행 상태, 리액션, 채팅
- **디바이스 설정 모달**: 마이크/스피커 선택, 테스트 녹음, 에코 캔슬 옵션
- **네트워크 상태 패널**: RTT/지터/패킷로스, 권장 안내(유선/헤드폰)

### 13.2 사용자 여정(핵심 플로우)

1. 로그인 → 로비 → 방 생성(노래방 모드)
2. 디바이스 설정 → 마이크 테스트 → MR 검색/선곡
3. 큐 승인/카운트다운 → 합창 시작 → 채팅/리액션
4. 일시정지/seek → 재동기화 → 종료/퇴장

---

## 14) API 초안(REST + STOMP)

### 14.1 REST(초안)

- `POST /api/auth/login`
- `POST /api/auth/logout`
- `GET /api/users/me`
- `GET /api/rooms`
- `POST /api/rooms`
- `GET /api/rooms/{roomId}`
- `POST /api/rooms/{roomId}/join`
- `POST /api/rooms/{roomId}/leave`
- `POST /api/rooms/{roomId}/kick`
- `GET /api/rooms/{roomId}/members`
- `GET /api/tracks/search?query=...`
- `POST /api/rooms/{roomId}/queue`
- `PATCH /api/rooms/{roomId}/queue/{queueItemId}`
- `GET /api/rooms/{roomId}/playback`
- `PATCH /api/rooms/{roomId}/playback`

### 14.2 STOMP(초안)

- `/topic/rooms/{roomId}/chat` (채팅)
- `/topic/rooms/{roomId}/presence` (입장/퇴장/준비 상태)
- `/topic/rooms/{roomId}/signal` (offer/answer/ice)
- `/topic/rooms/{roomId}/sync` (재생 시작/정지/seek/카운트다운)
- `/app/rooms/{roomId}/chat` (클라이언트 송신)
- `/app/rooms/{roomId}/signal` (시그널링 송신)
- `/app/rooms/{roomId}/sync` (동기화 송신)

---

## 15) DB 설계(초안)

### 15.1 테이블 요약

- `app_user` 사용자
- `room` 방
- `room_member` 방 참여자
- `chat_message` 채팅
- `track` MR 트랙
- `queue_item` 대기열
- `playback_state` 재생 상태
- `recording` 녹음(2차)

### 15.2 제약/인덱스 설계

- 사용자 이메일 유니크
- 방/유저 조합 유니크(중복 참여 방지)
- 방/채팅/큐 조회용 인덱스
- 상태 값은 enum 문자열로 저장(서버에서 검증)

### 15.3 로컬 H2 스키마

로컬 H2용 `schema.sql`과 샘플 데이터 `data.sql`을 준비한다.

---

## 16) 개발 일정(예시)

### 16.1 1~2주차: 기반 구축

- 프로젝트 세팅, 공통 레이아웃/라우팅
- WebSocket 연결/채팅, 방 CRUD
- H2 DB 스키마/샘플 데이터

### 16.2 3~4주차: 핵심 기능

- WebRTC 시그널링/오디오 송수신
- MR 큐/동기화 재생
- “마이크+MR” 믹싱 송출

### 16.3 5~6주차: 안정화/시연

- 네트워크 품질 표시/복구 UX
- 브라우저/장치 호환성 테스트
- 데모 시나리오 정리 및 발표 자료

---

## 17) 테스트 계획(초안)

### 17.1 기능 테스트

- 방 생성/입장/퇴장, 권한, 강퇴
- MR 검색/큐/동기화 재생
- 마이크 선택/테스트/송출

### 17.2 품질/호환성

- 브라우저(Chrome/Edge/Safari) 기본 시나리오
- 네트워크 변동(유선/와이파이 전환, 지연 시 복구)
- 헤드폰/스피커 전환, AEC/AGC 옵션

### 17.3 부하/동시성

- 2~6인 방 3개 동시 테스트
- 큐/채팅 이벤트 폭주 시 처리 확인

---

## 18) 운영/배포 관점(간단 정리)

- 개발: 로컬 H2 + 테스트 데이터
- 스테이징: Docker + Nginx + TURN 서버(coturn)
- 운영: 로그/지표 모니터링, 장애 대응 시나리오 문서화

---
